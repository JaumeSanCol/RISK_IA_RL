# An√°lisis del Log de Simulaci√≥n RL vs RL

**Archivo analizado:** `RISKGAME_RLVRL_20251210-201141.log`

## Resumen Ejecutivo

‚úÖ **Lo que funciona correctamente:**
- La simulaci√≥n se ejecuta sin errores
- Los logs se generan en formato v√°lido
- Las acciones se registran apropiadamente
- La l√≥gica de asignaci√≥n inicial de territorios funciona

‚ùå **Problemas encontrados:**

### 1. **Renombrado de Jugador Derrotado (CR√çTICO)**
**S√≠ntoma:** Un jugador se llama "Muerto" en lugar de su nombre original.

**Evidencia en el log:**
```
L√≠nea ~674:
RISKSTATE|"Muerto"$0$0$false$1673.6$0$0.1$true;"Rusia"$1$0$false$448.6$11$0.0$false;...
                                                                   ‚Üë
                                          defeated=true (jugador eliminado)

L√≠nea final:
RISKRESULT|Muerto,0.5|Rusia,0.5|Action Limit Reached|Turn Count = 154
```

**Causa probable:**
- En `clases/player.py` o en `risktools.py`, cuando un jugador es eliminado, su nombre se cambia a "Muerto"
- Esto afecta los logs, mostrando "Muerto" en lugar del nombre real del IA

**Impacto:**
- Los logs no reflejan correctamente qu√© jugador RL fue eliminado
- Hace imposible rastrear qu√© IA gan√≥/perdi√≥ en las estad√≠sticas

**Soluci√≥n recomendada:**
- NO cambiar el nombre del jugador en el objeto RiskPlayer
- Usar un campo `defeated=true` para marcar eliminaci√≥n, pero mantener el nombre original
- Modificar formato del log para incluir `defeated=true` sin cambiar nombre

### 2. **Resultado de Empate por Limit de Acciones (IMPORTANTE)**

**S√≠ntoma:** Partida termina con empate (0.5, 0.5) despu√©s de 5000 acciones ejecutadas.

**Evidencia:**
```
RISKRESULT|Muerto,0.5|Rusia,0.5|Action Limit Reached|Turn Count = 154
```

**An√°lisis:**
- 154 turnos / 2 jugadores = ~77 acciones por jugador en promedio
- 5000 acciones l√≠mite permitidas con 77 acciones por turno = sistema de acci√≥n est√° funcionando r√°pido
- **El problema es que sin ganador claro, ambos obtienen 0.5 puntos**

**Causa probable:**
- Los modelos RL entrenados no tienen comportamiento suficientemente agresivo
- Estrategia defensiva muy fuerte vs ofensiva d√©bil
- Ambos IAs est√°n igualados en defensa, ninguno logra breakthrough

**Impacto en entrenamiento:**
- Empates frecuentes reducen se√±al de aprendizaje
- Ambos modelos reciben recompensa similar aunque uno deber√≠a perder
- Puede causar convergencia d√©bil

### 3. **Anomal√≠a en Contador de Turnos (MENOR)**

**S√≠ntoma:** En log `RISKGAME_RLVRL_20251210-201256.log`, el contador de turnos salta de 12 a 1.

**Evidencia:**
```
L√≠nea 220: ...|"fase_3"|...|...|Turn Count = 12
L√≠nea 222: ...|"fase_1"|...|...|Turn Count = 1  ‚Üê Deber√≠a ser 13
```

**Causa probable:**
- Posible reinicio incorrecto de partida en el loop de torneo
- O la funci√≥n `play_match` no gestiona correctamente el reset entre partidas

**Impacto:**
- Logs confusos para an√°lisis post-mortem
- Posible bug en l√≥gica de torneos multi-partida

---

## Datos Estad√≠sticos

| M√©trica | Valor |
|---------|-------|
| Turnos Totales | 154 |
| L√≠mite de Acciones | 5000 |
| Resultado | Empate (0.5 cada uno) |
| Primera partida completada | S√≠ |

## Recomendaciones Prioritarias

### üî¥ Prioridad Cr√≠tica
1. **Fijar el problema de "Muerto"**: Mantener nombre original del jugador incluso cuando defeated=true
   - Archivo: `clases/player.py` 
   - Buscar d√≥nde se asigna `.name = "Muerto"`

### üü° Prioridad Alta  
2. **Investigar estrategia defensiva excesiva**:
   - Revisar recompensas en `risk_gym_env.py`
   - Evaluar si el modelo est√° siendo demasiado conservador
   - Posible falta de penalizaci√≥n a inactividad/defensa pura

3. **Validar contador de turnos**: 
   - Revisar funci√≥n `play_match()` en `play_rl_vs_rl.py`
   - Asegurar reset correcto entre partidas

### üü¢ Prioridad Media
4. **Aumentar agresividad del modelo**:
   - Ajustar reward shaping durante entrenamiento
   - Penalizar defender sin atacar
   - Recompensar expansi√≥n territorial

---

## Pr√≥ximos Pasos

1. **Inmediato:** Localizar y fijar el rename a "Muerto"
2. **Luego:** Ejecutar prueba simple con 1 partida para verificar nombres
3. **Despu√©s:** Revisar recompensas en environment gym
4. **Final:** Re-entrenar o ajustar modelos para mayor agresividad

