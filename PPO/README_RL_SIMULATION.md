# SimulaciÃ³n de Partidas: RL vs RL y RL vs HeurÃ­sticas

**DocumentaciÃ³n completa para simular partidas de RISK con modelos entrenados y visualizarlas en la GUI.**

---

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [Componentes](#componentes)
3. [InstalaciÃ³n](#instalaciÃ³n)
4. [Uso](#uso)
5. [Ejemplos PrÃ¡cticos](#ejemplos-prÃ¡cticos)
6. [Formato de Logs](#formato-de-logs)
7. [VisualizaciÃ³n en GUI](#visualizaciÃ³n-en-gui)
8. [Troubleshooting](#troubleshooting)

---

## DescripciÃ³n General

Este conjunto de herramientas permite:

âœ… **Enfrentar modelos RL entre sÃ­** y generar logs para visualizaciÃ³n  
âœ… **Enfrentar modelos RL contra IAs heurÃ­sticas** (clÃ¡sicas)  
âœ… **Generar logs en formato compatible** con `risk_game_viewer.py`  
âœ… **Recopilar estadÃ­sticas** de victorias y turnos  
âœ… **Modo verbose** para debugging  

### Flujo de Trabajo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Entrenar modelos PPO (train_ppo.py)                      â”‚
â”‚    â†’ Genera archivos .zip en logs_ppo/                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Simular partidas (play_rl_vs_rl.py o                     â”‚
â”‚    play_rl_vs_heuristics.py)                                â”‚
â”‚    â†’ Genera logs en logs/RISKGAME_*.log                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Visualizar en GUI (risk_game_viewer.py)                  â”‚
â”‚    â†’ Carga y reproduce logs de partidas                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Componentes

### 1. **`ppo_loader.py`** - Cargador de Modelos PPO

Proporciona la clase `PPOPlayer` que encapsula un modelo PPO entrenado.

**Responsabilidades:**
- Cargar archivos `.zip` del modelo
- Convertir estados de Risk a observaciones normalizadas
- Generar mÃ¡scaras de acciones vÃ¡lidas
- Decodificar predicciones del modelo a `RiskAction`
- Manejar fallbacks en caso de error

**API:**
```python
from ppo_loader import PPOPlayer

# Crear una IA RL
ppo_ai = PPOPlayer(
    model_path="logs_ppo/risk_ppo_aggressive_final.zip",
    player_name="RL-Agresivo"
)

# Usar en partida
state = risktools.getInitialState(board)
action = ppo_ai.getAction(state)  # Devuelve RiskAction
```

---

### 2. **`play_rl_vs_rl.py`** - Simulador RL vs RL

Enfrenta mÃºltiples modelos PPO entre sÃ­.

**CaracterÃ­sticas:**
- Soporta 2+ jugadores RL
- Genera logs automÃ¡ticos
- EstadÃ­sticas de victorias
- Modo verbose para debugging
- Compatible con GUI

**Uso:**
```bash
python play_rl_vs_rl.py <modelo1.zip> <modelo2.zip> [modelo3.zip ...] [opciones]
```

**Opciones:**
```
-n, --num N          NÃºmero de partidas donde cada RL es inicial (default: 5)
-w, --write          Guardar logs en logs/
-v, --verbose        Mostrar detalles durante ejecuciÃ³n
```

---

### 3. **`play_rl_vs_heuristics.py`** - Simulador RL vs HeurÃ­sticas

Enfrenta modelos PPO contra IAs heurÃ­sticas (y otros PPOs).

**CaracterÃ­sticas:**
- Mezcla libre de RL y heurÃ­sticas
- DetecciÃ³n automÃ¡tica de tipo por extensiÃ³n (`.zip` o `.py`)
- EstadÃ­sticas separadas por tipo de IA
- Modo verbose

**Uso:**
```bash
python play_rl_vs_heuristics.py <ia1> <ia2> [ia3 ...] [opciones]
```

Donde cada `<ia>` es:
- `ruta/modelo.zip` â†’ Modelo PPO
- `ruta/ai.py` â†’ IA heurÃ­stica

**Opciones:**
```
-n, --num N          NÃºmero de partidas donde cada IA es inicial (default: 5)
-w, --write          Guardar logs en logs/
-v, --verbose        Mostrar detalles durante ejecuciÃ³n
```

---

## InstalaciÃ³n

### Requisitos Previos

```bash
pip install stable-baselines3[extra]
pip install sb3-contrib
pip install gymnasium
pip install numpy
```

### Estructura de Directorios

```
RISK_IA_RL/
â”œâ”€â”€ PPO/
â”‚   â”œâ”€â”€ ppo_loader.py              â† Nuevo
â”‚   â”œâ”€â”€ play_rl_vs_rl.py            â† Nuevo
â”‚   â”œâ”€â”€ play_rl_vs_heuristics.py    â† Nuevo
â”‚   â”œâ”€â”€ logs_ppo/
â”‚   â”‚   â”œâ”€â”€ risk_ppo_aggressive_final.zip
â”‚   â”‚   â”œâ”€â”€ risk_ppo_defensive_final.zip
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ risk_gym_env.py
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ attacker_ai.py
â”‚   â”œâ”€â”€ random_ai.py
â”‚   â”œâ”€â”€ heuristic_ai.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/                          â† Logs generados aquÃ­
â”‚   â””â”€â”€ RISKGAME_*.log
â”œâ”€â”€ risktools.py
â”œâ”€â”€ play_risk_ai.py               â† Script antiguo (para referencia)
â””â”€â”€ risk_game_viewer.py           â† GUI para visualizar
```

---

## Uso

### Escenario 1: RL vs RL

Enfrenta 2 modelos entrenados en 3 partidas cada uno como jugador inicial.

```bash
cd PPO
python play_rl_vs_rl.py \
    logs_ppo/risk_ppo_aggressive_final.zip \
    logs_ppo/risk_ppo_defensive_final.zip \
    -n 3 \
    -w \
    -v
```

**Salida esperada:**
```
[CARGA] Cargando tablero...
[CARGA] Cargando 2 modelos PPO...

  [PPOPlayer] Modelo cargado: Castilla
  [PPOPlayer] Modelo cargado: AragÃ³n

[TORNEO] Iniciando torneo de 3 partidas...

[PARTIDA 1/3] Orden: ['Castilla', 'AragÃ³n']
[TURNO 0] Jugador: Castilla | Tipo: Comprar_Soldados | Tiempo: 600.0s
  AcciÃ³n: Place IN: Argentina NUM: 2
  Tiempo: 0.052s
...
[FIN] Castilla ganÃ³ la partida!
  Log guardado: logs/RISKGAME_RLVRL_20241210-153045.log

[ESTADÃSTICAS DEL TORNEO RL vs RL]
  PARTIDAS JUGADAS  : 3
  VICTORIAS NORMALES: 2
  EMPATES           : 1
  TURNOS PROMEDIO   : 45.33
  
  VICTORIAS POR JUGADOR:
    Castilla: 2 victorias
    AragÃ³n: 1 victorias
```

---

### Escenario 2: RL vs HeurÃ­sticas

Enfrenta 1 modelo RL contra 2 IAs clÃ¡sicas.

```bash
cd PPO
python play_rl_vs_heuristics.py \
    logs_ppo/risk_ppo_aggressive_final.zip \
    ../ai/attacker_ai.py \
    ../ai/random_ai.py \
    -n 4 \
    -w \
    -v
```

**Salida esperada:**
```
[CARGA] Cargando tablero...
[CARGA] Cargando 3 IAs (PPO + HeurÃ­sticas)...

  âœ“ PPO cargado: Prusia
  âœ“ HeurÃ­stica cargada: Baviera (attacker_ai)
  âœ“ HeurÃ­stica cargada: Bohemia (random_ai)

[TORNEO] Iniciando torneo de 4 partidas...

[PARTIDA 1/4] Orden: ['Prusia', 'Baviera', 'Bohemia']
  1. Prusia (PPO)
  2. Baviera (Heuristic)
  3. Bohemia (Heuristic)

[TURNO 0] Prusia (PPO) | Tipo: Comprar_Soldados | Tiempo: 600.0s
...
[FIN] Prusia ganÃ³!
  Log guardado: logs/RISKGAME_RLVSHEUR_20241210-153125.log

[ESTADÃSTICAS DEL TORNEO RL vs HEURÃSTICAS]
  PARTIDAS JUGADAS  : 4
  VICTORIAS NORMALES: 3
  EMPATES           : 1
  TURNOS PROMEDIO   : 48.50
  
  VICTORIAS POR JUGADOR:
    [PPO       ] Prusia: 3 victorias
    [Heuristic] Baviera: 1 victorias
    [Heuristic] Bohemia: 0 victorias
```

---

### Escenario 3: Solo generaciÃ³n de logs (sin verbose)

```bash
cd PPO
python play_rl_vs_rl.py \
    logs_ppo/risk_ppo_aggressive.zip \
    logs_ppo/risk_ppo_defensive.zip \
    -n 10 \
    -w
```

Se ejecutarÃ¡ silenciosamente y guardarÃ¡ 10 partidas en `logs/`.

---

## Ejemplos PrÃ¡cticos

### Ejemplo 1: Comparar 3 modelos diferentes

```bash
python play_rl_vs_rl.py \
    logs_ppo/MaskablePPO_5/risk_ppo_aggressive.zip \
    logs_ppo/MaskablePPO_6/risk_ppo_defensive.zip \
    logs_ppo/MaskablePPO_7/risk_ppo_balanced.zip \
    -n 6 -w
```

Esto ejecutarÃ¡ **18 partidas** en total (6 por modelo como jugador inicial) y las guardarÃ¡ en logs/.

---

### Ejemplo 2: RL vs HeurÃ­sticas con mÃºltiples IAs

```bash
python play_rl_vs_heuristics.py \
    logs_ppo/risk_ppo_aggressive_final.zip \
    ../ai/attacker_ai.py \
    ../ai/heuristic_ai.py \
    ../ai/random_ai.py \
    -n 5 -w -v
```

**Resultado:** Un torneo de 5Ã—4=20 partidas con 1 RL vs 3 heurÃ­sticas.

---

### Ejemplo 3: Prueba rÃ¡pida en modo verbose

```bash
python play_rl_vs_heuristics.py \
    logs_ppo/risk_ppo_aggressive_final.zip \
    ../ai/random_ai.py \
    -n 1 -v
```

Ejecuta **1 partida** con mÃ¡ximo detalle, sin guardar logs.

---

## Formato de Logs

Los logs se generan en formato **idÃ©ntico a `play_risk_ai.py`** para compatibilidad total con la GUI.

### Estructura del Archivo Log

```
RISKBOARD|...
RISKSTATE|...
RISKACTION|"Attack"|"Argentina"|"Brasil"|null|null|null
RISKSTATE|...
RISKACTION|"Fortify"|null|"Canada"|5|null|null
RISKSTATE|...
...
RISKSTATE|...
RISKRESULT|NombreCampeÃ³n,1|OtroJugador,0|..."Game End"|Turn Count = 45
```

### Campos de AcciÃ³n

```python
RISKACTION | type | to_territory | from_territory | unidades | to_player | from_player
```

**Ejemplo descodificado:**
- `type`: "Attack", "Fortify", "Place", "Occupy", "Pasar", etc.
- `to_territory`: ID o nombre del territorio destino
- `from_territory`: ID o nombre del territorio origen
- `unidades`: NÃºmero de unidades involucradas
- `to_player`: ID del jugador destino (para acciones entre jugadores)
- `from_player`: ID del jugador origen

---

## VisualizaciÃ³n en GUI

Una vez generados los logs, puedes visualizarlos en la GUI:

```bash
python risk_game_viewer.py logs/RISKGAME_RLVRL_20241210-153045.log
```

La GUI reproducirÃ¡ la partida paso a paso, mostrando:
- Estado del tablero despuÃ©s de cada acciÃ³n
- Soldados y dinero de cada jugador
- Turno y tipo de acciÃ³n
- Historial completo de la partida

---

## Troubleshooting

### Error: `FileNotFoundError: Modelo no encontrado`

**Causa:** La ruta del modelo es incorrecta.

**SoluciÃ³n:**
```bash
# AsegÃºrate de que el archivo .zip existe
ls logs_ppo/

# Usa ruta relativa desde PPO/ o ruta absoluta
python play_rl_vs_rl.py logs_ppo/modelo.zip -w
```

---

### Error: `ModuleNotFoundError: No module named 'risktools'`

**Causa:** Ejecutas el script desde fuera del directorio `PPO/`.

**SoluciÃ³n:**
```bash
cd PPO
python play_rl_vs_rl.py [modelos] -w
```

O ajusta `sys.path` en el script.

---

### Error: `ImportError: cannot import name 'RiskTotalControlEnv'`

**Causa:** El archivo `risk_gym_env.py` no estÃ¡ en el directorio `PPO/`.

**SoluciÃ³n:**
```bash
# Verifica que exista
ls PPO/risk_gym_env.py

# Si no existe, cÃ³pialo desde el backup
cp PPO/backup_ia/risk_gym_env.py PPO/
```

---

### Las IAs juegan acciones invÃ¡lidas constantemente

**Causa:** El modelo no se cargÃ³ correctamente o hay incompatibilidad.

**SoluciÃ³n:**
1. Recarga el modelo desde cero
2. Prueba en modo `deterministic=False` en `ppo_loader.py` para exploraciÃ³n
3. Revisa que el modelo se entrenÃ³ con `n_players=N` igual al nÃºmero de jugadores

---

### Logs muy cortos (solo 1-2 turnos)

**Causa:** El modelo colapsa o hay error en la decodificaciÃ³n de acciones.

**SoluciÃ³n:**
```bash
# Prueba con -v para ver dÃ³nde falla
python play_rl_vs_rl.py logs_ppo/modelo.zip -n 1 -v

# Si hay muchos "ADVERTENCIA acciÃ³n invÃ¡lida", el modelo necesita reentrenamiento
```

---

### Â¿CÃ³mo aumentar velocidad de simulaciÃ³n?

Edita en `play_rl_vs_rl.py`:

```python
# Antes (verbose)
python play_rl_vs_rl.py modelo1.zip modelo2.zip -n 100 -w

# DespuÃ©s (sin verbose, mÃºltiples en paralelo si tienes CPU)
python play_rl_vs_rl.py modelo1.zip modelo2.zip -n 100 -w &
python play_rl_vs_rl.py modelo3.zip modelo4.zip -n 100 -w &
```

---

## API Reference

### PPOPlayer

```python
class PPOPlayer:
    def __init__(self, model_path: str, player_name: str = "RL-Agent", device: str = 'cpu')
    def getAction(self, state: RiskState) -> RiskAction
```

**MÃ©todos privados (uso interno):**
- `_resolve_model_path(model_path)` - Resuelve rutas relativas
- `_load_model()` - Carga el archivo .zip
- `_decode_action_from_encoded(state, action)` - Decodifica predicciÃ³n
- `_get_random_action(state)` - Fallback aleatorio

---

## ConfiguraciÃ³n Avanzada

### Cambiar Tiempo LÃ­mite por Jugador

En `play_rl_vs_rl.py`, lÃ­nea ~185:

```python
time_left = {i: 300.0 for i in range(len(player_names))}  # 5 minutos en lugar de 10
```

### Cambiar LÃ­mite de Acciones

En `play_rl_vs_rl.py`, lÃ­nea ~186:

```python
action_limit = 10000  # Permitir mÃ¡s turnos
```

### Usar GPU para Inferencia

En `ppo_loader.py`, lÃ­nea ~50:

```python
ppo = PPOPlayer(model_path, device='cuda')  # En lugar de 'cpu'
```

---

## Preguntas Frecuentes

**P: Â¿Puedo entrenar mientras simulo?**  
R: SÃ­, los scripts de simulaciÃ³n no interfieren con `train_ppo.py`. Usa diferentes procesos/terminales.

**P: Â¿Los logs son deterministicos?**  
R: Casi (determinstic=True en modelo), pero algunas acciones tienen probabilidades inherentes en Risk (dados de batalla).

**P: Â¿CÃ³mo comparo dos modelos?**  
R: EnfrÃ©ntalos muchas veces: `python play_rl_vs_rl.py model1.zip model2.zip -n 50 -w`

**P: Â¿Debo usar nombres especÃ­ficos para jugadores?**  
R: No, se asignan automÃ¡ticamente de una lista de reinos reales. Puedes editarla en el script.

---

## Notas Finales

- Los logs son **100% compatibles** con `risk_game_viewer.py`
- La velocidad depende de tu CPU; modelos complejos son mÃ¡s lentos
- Para anÃ¡lisis estadÃ­stico, extrae datos de la salida del script o parsea los logs
- Puedes combinar RL + heurÃ­sticas en `play_rl_vs_heuristics.py` sin lÃ­mite

---

**Ãšltima actualizaciÃ³n:** Diciembre 2024  
**Autor:** Sistema de SimulaciÃ³n RL para RISK
